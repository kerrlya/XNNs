{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VR0wDf37QZYZ"
   },
   "source": [
    "# **RNN Classifier for Tweet Data**\n",
    "\n",
    "This notebook trains the model for an RNN classifier of our data. Word vectorization and recurrent neural network setup followed from youtube.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2GBA-LXTqDB"
   },
   "source": [
    "First install old version of torchtext because new ones have issues with legacy tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cDsulUpWRlWH",
    "outputId": "e1e4c5e6-2956-4320-8e42-fbbd5d0096d5"
   },
   "outputs": [],
   "source": [
    "# !conda install -c pytorch torcht ext==0.6.0\n",
    "# !conda install pytorch::pytorch torchvision torchaudio -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKOTkda4T5J4"
   },
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gCbYs3GrTxCI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvdxN8P4UAGs"
   },
   "source": [
    "Settings for different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FPeUlIEJT9n5"
   },
   "outputs": [],
   "source": [
    "# select regularization type and strength\n",
    "L1_REG = False\n",
    "l1_lambda = 0.01\n",
    "\n",
    "L2_REG = False\n",
    "l2_lambda = 0.01\n",
    "\n",
    "# select data balancing choice\n",
    "choice = \"deletion\"\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "VOCABULARY_SIZE = 20000\n",
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 15\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_CLASSES = 2\n",
    "DATA_PATH = \"./tweets.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXDexYm4ULte"
   },
   "source": [
    "## Setup and Format Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVMahw_xxZJk"
   },
   "source": [
    "Balance data by either duplication or deletion (to prevent overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t5rixSfgxfBl",
    "outputId": "7b58d900-5499-4196-99cc-b1c1e7e5ce1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    8816\n",
      "1    8816\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"./tweets.csv\")\n",
    "\n",
    "# Count the occurrences of each label\n",
    "label_counts = df['label'].value_counts()\n",
    "\n",
    "# Count minority labels\n",
    "minority_label = label_counts.idxmin()\n",
    "minority_count = label_counts.min()\n",
    "\n",
    "# Find indices of the majority class to delete excess samples\n",
    "majority_indices = df[df['label'] != minority_label].index\n",
    "excess_majority_indices = majority_indices[minority_count:]\n",
    "\n",
    "if choice == \"deletion\":\n",
    "  # Delete excess samples from the majority class to balance the dataset\n",
    "  balanced_df = df.drop(excess_majority_indices)\n",
    "  balanced_df.to_csv(\"./tweetsDEL.csv\", index=False)\n",
    "  DATA_PATH = \"./tweetsDEL.csv\"\n",
    "if choice == \"duplication\":\n",
    "  duplicate_indices = pd.Series(minority_indices).sample(n=abs(label_counts.diff().values[0]), replace=True).values\n",
    "  duplicated_samples = df.loc[duplicate_indices]\n",
    "  balanced_df = df.append(duplicated_samples, ignore_index=True)\n",
    "  balanced_df.to_csv(\"./tweetsDUP.csv\", index=False)\n",
    "  DATA_PATH = \"./tweetsDUP.csv\"\n",
    "\n",
    "# Verify the new class distribution\n",
    "print(balanced_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnY04moYWE4X"
   },
   "source": [
    "Define text and label formatters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7DAVzmWaVOPv"
   },
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "TEXT = torchtext.data.Field(tokenize='spacy', tokenizer_language='en_core_web_sm')\n",
    "LABEL = torchtext.data.LabelField(dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dyUqipHWZz1"
   },
   "source": [
    "Process/format data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ncNIvzDEWYWZ"
   },
   "outputs": [],
   "source": [
    "fields = [(\"text\", TEXT), (\"label\", LABEL)]\n",
    "dataset = torchtext.data.TabularDataset(\n",
    "    path=DATA_PATH, format='csv',\n",
    "    skip_header=True, fields=fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdDi0VXiXfGG"
   },
   "source": [
    "## Split Dataset into Test/Train/Validation sets\n",
    "\n",
    "Test/Train split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8cxwbtuAXtiI"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = dataset.split(\n",
    "    split_ratio=[0.8, 0.2],\n",
    "    random_state=random.seed(RANDOM_SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYZ2iR2bYEEh"
   },
   "source": [
    "Split train into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "i9Z7GlQsXwhw"
   },
   "outputs": [],
   "source": [
    "train_data, valid_data = train_data.split(\n",
    "    split_ratio=[0.8, 0.20],\n",
    "    random_state=random.seed(RANDOM_SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmYEEaXTYYki"
   },
   "source": [
    "Show sizes of sets and example text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVzh7GruYIfe",
    "outputId": "25ad5fa9-8004-4fd6-d8c9-a2ffb0f08011"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Size: 11285\n",
      "Train Size: 11285\n",
      "Validation Size: 2821\n",
      "{'text': ['#', '63millionbabies', ' ', 'who', 'have', 'been', 'killed', 'by', 'ROE', '\\n', '#', 'RoeVWade', '\\n', '#', 'SupremeCourt'], 'label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(f'Test Size: {len(train_data)}')\n",
    "print(f'Train Size: {len(train_data)}')\n",
    "print(f'Validation Size: {len(valid_data)}')\n",
    "print(vars(train_data.examples[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yoz-35XYe-l"
   },
   "source": [
    "## Build Vocabulary\n",
    "Valid words are the top frequent VOCABULARY_SIZE words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_PEScKVYYyln",
    "outputId": "fdd4f3da-b0c4-4e1d-8fcf-8f0ef6a8a361"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20002\n",
      "Label size: 2\n",
      "[('.', 19380), ('the', 12468), ('to', 11163), (',', 10679), ('abortion', 7932), ('a', 6473), ('of', 6409), ('is', 6252), ('#', 6205), ('and', 6201), ('in', 4539), ('I', 4154), ('\\n\\n', 3974), ('that', 3902), ('for', 3884), ('!', 3678), ('\\n', 3630), ('you', 2917), ('are', 2868), ('it', 2793)]\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, max_size=VOCABULARY_SIZE)\n",
    "LABEL.build_vocab(train_data, max_size = 2)\n",
    "print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
    "print(f'Label size: {len(LABEL.vocab)}')\n",
    "print(TEXT.vocab.freqs.most_common(20)) # most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MRbTiIsRZhXZ",
    "outputId": "54f29db2-173c-41c4-b853-0a525b6e05db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 19380), ('the', 12468), ('to', 11163), (',', 10679), ('abortion', 7932), ('a', 6473), ('of', 6409), ('is', 6252), ('#', 6205), ('and', 6201), ('in', 4539), ('I', 4154), ('\\n\\n', 3974), ('that', 3902), ('for', 3884), ('!', 3678), ('\\n', 3630), ('you', 2917), ('are', 2868), ('it', 2793)]\n",
      "['<unk>', '<pad>', '.', 'the', 'to', ',', 'abortion', 'a', 'of', 'is']\n",
      "3\n",
      "defaultdict(None, {'1': 0, '0': 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'1': 5694, '0': 5591})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))\n",
    "print(TEXT.vocab.itos[:10])\n",
    "print(TEXT.vocab.stoi['the'])\n",
    "print(LABEL.vocab.stoi)\n",
    "LABEL.vocab.freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnvujU5yZ9uS"
   },
   "source": [
    "## Data Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aFjOWgWnaBSW"
   },
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = \\\n",
    "    torchtext.data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data),\n",
    "         batch_size=BATCH_SIZE,\n",
    "         sort_within_batch=False,\n",
    "         sort_key=lambda x: len(x.text),\n",
    "         device=DEVICE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p_SUWBdzaBYj",
    "outputId": "75a71276-c0f7-4ec2-f9bb-00ea76990e8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Text matrix size: torch.Size([67, 128])\n",
      "Target vector size: torch.Size([128])\n",
      "\n",
      "Valid:\n",
      "Text matrix size: torch.Size([11, 128])\n",
      "Target vector size: torch.Size([128])\n",
      "\n",
      "Test:\n",
      "Text matrix size: torch.Size([10, 128])\n",
      "Target vector size: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "for batch in train_loader:\n",
    "    print(f'Text matrix size: {batch.text.size()}')\n",
    "    print(f'Target vector size: {batch.label.size()}')\n",
    "    break\n",
    "\n",
    "print('\\nValid:')\n",
    "for batch in valid_loader:\n",
    "    print(f'Text matrix size: {batch.text.size()}')\n",
    "    print(f'Target vector size: {batch.label.size()}')\n",
    "    break\n",
    "\n",
    "print('\\nTest:')\n",
    "for batch in test_loader:\n",
    "    print(f'Text matrix size: {batch.text.size()}')\n",
    "    print(f'Target vector size: {batch.label.size()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkIpjCZaafi3"
   },
   "source": [
    "## Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WthFTcb_aazO"
   },
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        # self.rnn = torch.nn.RNN(embedding_dim,\n",
    "        #                        hidden_dim,\n",
    "        #                        nonlinearity='relu')\n",
    "        self.rnn = torch.nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim)\n",
    "\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text dim: [sentence length, batch size]\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded dim: [sentence length, batch size, embedding dim]\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        # output dim: [sentence length, batch size, hidden dim]\n",
    "        # hidden dim: [1, batch size, hidden dim]\n",
    "\n",
    "        hidden.squeeze_(0)\n",
    "        # hidden dim: [batch size, hidden dim]\n",
    "\n",
    "        output = self.fc(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "RTCuyeZbadh8"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = RNN(input_dim=len(TEXT.vocab),\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            output_dim=NUM_CLASSES # could use 1 for binary classification\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005) # use weight decay for l2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BD5YIU6zai-8"
   },
   "source": [
    "## Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8du5AFqHak0Y"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct_pred, num_examples = 0, 0\n",
    "\n",
    "        for i, (features, targets) in enumerate(data_loader):\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets.float().to(device)\n",
    "\n",
    "            logits = model(features)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qiWvVMODatCp",
    "outputId": "26f510b3-b90b-400d-fb4d-8eaa651ae479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/015 | Batch 000/089 | Loss: 0.6973\n",
      "Epoch: 001/015 | Batch 050/089 | Loss: 0.6933\n",
      "training accuracy: 50.67%\n",
      "valid accuracy: 49.73%\n",
      "Time elapsed: 0.25 min\n",
      "Epoch: 002/015 | Batch 000/089 | Loss: 0.6928\n",
      "Epoch: 002/015 | Batch 050/089 | Loss: 0.6906\n",
      "training accuracy: 49.53%\n",
      "valid accuracy: 48.85%\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 003/015 | Batch 000/089 | Loss: 0.6916\n",
      "Epoch: 003/015 | Batch 050/089 | Loss: 0.6908\n",
      "training accuracy: 50.69%\n",
      "valid accuracy: 50.55%\n",
      "Time elapsed: 0.76 min\n",
      "Epoch: 004/015 | Batch 000/089 | Loss: 0.6991\n",
      "Epoch: 004/015 | Batch 050/089 | Loss: 0.6933\n",
      "training accuracy: 54.84%\n",
      "valid accuracy: 48.42%\n",
      "Time elapsed: 1.01 min\n",
      "Epoch: 005/015 | Batch 000/089 | Loss: 0.7006\n",
      "Epoch: 005/015 | Batch 050/089 | Loss: 0.6997\n",
      "training accuracy: 56.44%\n",
      "valid accuracy: 54.63%\n",
      "Time elapsed: 1.26 min\n",
      "Epoch: 006/015 | Batch 000/089 | Loss: 0.6836\n",
      "Epoch: 006/015 | Batch 050/089 | Loss: 0.6901\n",
      "training accuracy: 56.73%\n",
      "valid accuracy: 55.51%\n",
      "Time elapsed: 1.51 min\n",
      "Epoch: 007/015 | Batch 000/089 | Loss: 0.6684\n",
      "Epoch: 007/015 | Batch 050/089 | Loss: 0.6768\n",
      "training accuracy: 60.25%\n",
      "valid accuracy: 53.92%\n",
      "Time elapsed: 1.77 min\n",
      "Epoch: 008/015 | Batch 000/089 | Loss: 0.6663\n",
      "Epoch: 008/015 | Batch 050/089 | Loss: 0.6619\n",
      "training accuracy: 61.52%\n",
      "valid accuracy: 58.49%\n",
      "Time elapsed: 2.02 min\n",
      "Epoch: 009/015 | Batch 000/089 | Loss: 0.6558\n",
      "Epoch: 009/015 | Batch 050/089 | Loss: 0.6183\n",
      "training accuracy: 70.94%\n",
      "valid accuracy: 63.95%\n",
      "Time elapsed: 2.29 min\n",
      "Epoch: 010/015 | Batch 000/089 | Loss: 0.4483\n",
      "Epoch: 010/015 | Batch 050/089 | Loss: 0.5292\n",
      "training accuracy: 75.08%\n",
      "valid accuracy: 65.83%\n",
      "Time elapsed: 2.55 min\n",
      "Epoch: 011/015 | Batch 000/089 | Loss: 0.4953\n",
      "Epoch: 011/015 | Batch 050/089 | Loss: 0.4450\n",
      "training accuracy: 83.67%\n",
      "valid accuracy: 69.73%\n",
      "Time elapsed: 2.80 min\n",
      "Epoch: 012/015 | Batch 000/089 | Loss: 0.3335\n",
      "Epoch: 012/015 | Batch 050/089 | Loss: 0.5132\n",
      "training accuracy: 88.99%\n",
      "valid accuracy: 70.44%\n",
      "Time elapsed: 3.06 min\n",
      "Epoch: 013/015 | Batch 000/089 | Loss: 0.2955\n",
      "Epoch: 013/015 | Batch 050/089 | Loss: 0.2650\n",
      "training accuracy: 92.12%\n",
      "valid accuracy: 72.31%\n",
      "Time elapsed: 3.33 min\n",
      "Epoch: 014/015 | Batch 000/089 | Loss: 0.2308\n",
      "Epoch: 014/015 | Batch 050/089 | Loss: 0.2320\n",
      "training accuracy: 94.60%\n",
      "valid accuracy: 73.20%\n",
      "Time elapsed: 3.60 min\n",
      "Epoch: 015/015 | Batch 000/089 | Loss: 0.0837\n",
      "Epoch: 015/015 | Batch 050/089 | Loss: 0.1697\n",
      "training accuracy: 95.67%\n",
      "valid accuracy: 74.51%\n",
      "Time elapsed: 3.86 min\n",
      "Total Training Time: 3.86 min\n",
      "Test accuracy: 72.66%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "\n",
    "        text = batch_data.text.to(DEVICE)\n",
    "        labels = batch_data.label.to(DEVICE)\n",
    "\n",
    "        ### FORWARD AND BACK PROP\n",
    "\n",
    "        logits = model(text)\n",
    "\n",
    "        # regularization (optional)\n",
    "        loss = F.cross_entropy(logits, labels) # cross entropy loss tends to be better for classification problems\n",
    "        #if L1_REG:\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "\n",
    "        # LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
    "\n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "\n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMdbrX2QbH1g"
   },
   "source": [
    "Test on basic in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uc5HQPCwbDeI",
    "outputId": "ceca9410-f2d2-4e99-d2ee-d22ddfe77971"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def predict_side(model, sentence):\n",
    "\n",
    "    model.eval()\n",
    "    tokenized = [token.text for token in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(DEVICE)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.nn.functional.softmax(model(tensor), dim=1)\n",
    "    return prediction[0][0].item()\n",
    "\n",
    "# print('Probability Pro-Life:')\n",
    "print(\"{:75} {}\".format(\"Prompt\", \"Output\"))\n",
    "print(\"{:75} {}\".format(\"women's rights are so important\", predict_side(model,\"women's rights are so important\")))\n",
    "print(\"{:75} {}\".format(\"God loves babies thank god \", predict_side(model, \"God loves babies thank god \")))\n",
    "print(\"{:75} {}\".format(\"We have cats\", predict_side(model, \"We have cats\")))\n",
    "print(\"{:75} {}\".format(\"Potatoes\", predict_side(model, \"Potatoes\")))\n",
    "print(\"{:75} {}\".format(\"According to all known laws of aviation, a bee should not be able to fly.\", predict_side(model, \"According to all known laws of aviation, a bee should not be able to fly.\")))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt                                                                                Output\n",
      "brap brap pew pew                                                                     0.6179547309875488\n",
      "brap brap pew pew cats                                                                0.18592871725559235\n",
      "brap brap pew pew cats cats                                                           0.029808202758431435\n",
      "brap brap pew pew cats cats cats                                                      0.021975139155983925\n",
      "brap brap pew pew cats cats cats cats                                                 0.018449626863002777\n",
      "brap brap pew pew cats cats cats cats cats                                            0.016453083604574203\n",
      "brap brap pew pew cats cats cats cats cats cats                                       0.015526123344898224\n",
      "brap brap pew pew cats cats cats cats cats cats cats                                  0.015238828957080841\n",
      "brap brap pew pew cats cats cats cats cats cats cats cats                             0.01519849430769682\n",
      "brap brap pew pew cats cats cats cats cats cats cats cats cats                        0.015179944224655628\n"
     ]
    }
   ],
   "source": [
    "print(\"{:85} {}\".format(\"Prompt\", \"Output\"))\n",
    "for cat_count in range(10):\n",
    "    prompt = f\"brap brap pew pew \" + (\"cats \" * cat_count)\n",
    "    result = predict_side(model, prompt)\n",
    "    print(\"{:85} {}\".format(prompt, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "hDyXftyQrvPc"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'RNN_deletion.pth')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
